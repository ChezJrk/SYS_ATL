def rank_k_reduce_6x16_scheduled(K: size, C: [f32][6, 16] @ DRAM,
                                 A: [f32][6, K] @ DRAM,
                                 B: [f32][K, 16] @ DRAM):
    C_reg: R[6, 2, 8] @ AVX2
    mm256_loadu_ps(C_reg[0, 0, 0:8], C[0, 0:8])
    mm256_loadu_ps(C_reg[0, 1, 0:8], C[0, 8:16])
    mm256_loadu_ps(C_reg[1, 0, 0:8], C[1, 0:8])
    mm256_loadu_ps(C_reg[1, 1, 0:8], C[1, 8:16])
    mm256_loadu_ps(C_reg[2, 0, 0:8], C[2, 0:8])
    mm256_loadu_ps(C_reg[2, 1, 0:8], C[2, 8:16])
    mm256_loadu_ps(C_reg[3, 0, 0:8], C[3, 0:8])
    mm256_loadu_ps(C_reg[3, 1, 0:8], C[3, 8:16])
    mm256_loadu_ps(C_reg[4, 0, 0:8], C[4, 0:8])
    mm256_loadu_ps(C_reg[4, 1, 0:8], C[4, 8:16])
    mm256_loadu_ps(C_reg[5, 0, 0:8], C[5, 0:8])
    mm256_loadu_ps(C_reg[5, 1, 0:8], C[5, 8:16])
    for k in par(0, K):
        a_vec: R[8] @ AVX2
        mm256_broadcast_ss(a_vec, A[0, k:k + 1])
        b_vec: R[8] @ AVX2
        mm256_loadu_ps(b_vec[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[0, 0, 0:8], a_vec, b_vec)
        a_vec_1: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_1, A[0, k:k + 1])
        b_vec_1: R[8] @ AVX2
        mm256_loadu_ps(b_vec_1[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[0, 1, 0:8], a_vec_1, b_vec_1)
        a_vec_2: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_2, A[1, k:k + 1])
        b_vec_2: R[8] @ AVX2
        mm256_loadu_ps(b_vec_2[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[1, 0, 0:8], a_vec_2, b_vec_2)
        a_vec_3: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_3, A[1, k:k + 1])
        b_vec_3: R[8] @ AVX2
        mm256_loadu_ps(b_vec_3[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[1, 1, 0:8], a_vec_3, b_vec_3)
        a_vec_4: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_4, A[2, k:k + 1])
        b_vec_4: R[8] @ AVX2
        mm256_loadu_ps(b_vec_4[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[2, 0, 0:8], a_vec_4, b_vec_4)
        a_vec_5: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_5, A[2, k:k + 1])
        b_vec_5: R[8] @ AVX2
        mm256_loadu_ps(b_vec_5[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[2, 1, 0:8], a_vec_5, b_vec_5)
        a_vec_6: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_6, A[3, k:k + 1])
        b_vec_6: R[8] @ AVX2
        mm256_loadu_ps(b_vec_6[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[3, 0, 0:8], a_vec_6, b_vec_6)
        a_vec_7: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_7, A[3, k:k + 1])
        b_vec_7: R[8] @ AVX2
        mm256_loadu_ps(b_vec_7[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[3, 1, 0:8], a_vec_7, b_vec_7)
        a_vec_8: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_8, A[4, k:k + 1])
        b_vec_8: R[8] @ AVX2
        mm256_loadu_ps(b_vec_8[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[4, 0, 0:8], a_vec_8, b_vec_8)
        a_vec_9: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_9, A[4, k:k + 1])
        b_vec_9: R[8] @ AVX2
        mm256_loadu_ps(b_vec_9[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[4, 1, 0:8], a_vec_9, b_vec_9)
        a_vec_10: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_10, A[5, k:k + 1])
        b_vec_10: R[8] @ AVX2
        mm256_loadu_ps(b_vec_10[0:8], B[k, 0:8])
        mm256_fmadd_ps(C_reg[5, 0, 0:8], a_vec_10, b_vec_10)
        a_vec_11: R[8] @ AVX2
        mm256_broadcast_ss(a_vec_11, A[5, k:k + 1])
        b_vec_11: R[8] @ AVX2
        mm256_loadu_ps(b_vec_11[0:8], B[k, 8:16])
        mm256_fmadd_ps(C_reg[5, 1, 0:8], a_vec_11, b_vec_11)
    mm256_storeu_ps(C[0, 0:8], C_reg[0, 0, 0:8])
    mm256_storeu_ps(C[0, 8:16], C_reg[0, 1, 0:8])
    mm256_storeu_ps(C[1, 0:8], C_reg[1, 0, 0:8])
    mm256_storeu_ps(C[1, 8:16], C_reg[1, 1, 0:8])
    mm256_storeu_ps(C[2, 0:8], C_reg[2, 0, 0:8])
    mm256_storeu_ps(C[2, 8:16], C_reg[2, 1, 0:8])
    mm256_storeu_ps(C[3, 0:8], C_reg[3, 0, 0:8])
    mm256_storeu_ps(C[3, 8:16], C_reg[3, 1, 0:8])
    mm256_storeu_ps(C[4, 0:8], C_reg[4, 0, 0:8])
    mm256_storeu_ps(C[4, 8:16], C_reg[4, 1, 0:8])
    mm256_storeu_ps(C[5, 0:8], C_reg[5, 0, 0:8])
    mm256_storeu_ps(C[5, 8:16], C_reg[5, 1, 0:8])
